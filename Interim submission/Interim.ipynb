{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1.1 </h1>\n",
    "<body>\n",
    "<font size = \"2\" color = \"red\">->Put raw csv in same path as ipynb for smooth running of code </font><br><br>\n",
    "\n",
    "<font size = \"3\">This part of the code finds the number of houses in to whole dataset,<br>\n",
    "outputs <b><i>clean_df</i></b> csv for further processing,<br>\n",
    "marks out the time period of all malfunction meter</font><br><br>\n",
    "\n",
    "In 1.1 we are supposed to generate a collections malfunction. We will first need to define what data points are counted as<br> malfunction, then we need to extract them. We have defined malfunction to be of these types:<br><br>\n",
    " 1: data reported back when change in gas use is < 2 cubic foot<br>\n",
    " 2: data reported back new meter_value is smaller than old meter_value<br>\n",
    " 3: data reported back is >2, but time > 15s, threshold value for reporting malfunction will be 7 cubic foot/hr<br> \n",
    " (US household average daily usage = 168 cubic foot)<br>\n",
    " 4: However, continuous report of same reading from same id over 12hrs,<br>\n",
    " the first reading after 12hrs will be treated as good reading,<br>\n",
    " as the gas company will probably want to know whether is a meter malfunctioning<br>\n",
    " even though its reading did not change for a period of time<br><br>\n",
    "\n",
    "And these are the main problems that we encountered,<br>\n",
    "- Finding efficient method to loop through the whole dataset, extracting what we want<br>\n",
    "- Looping through the dataset, removing what we don't want.<br>\n",
    "\n",
    "</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'C:\\\\Users\\\\Administrator\\\\ee4211\\\\4211\\\\Interim submission./dataport-export_gas_oct2015-mar2016.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7ad1c3991ac0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Read in raw data and find\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mdata_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dataid'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of houses =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Coding\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Coding\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Coding\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Coding\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Coding\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'C:\\\\Users\\\\Administrator\\\\ee4211\\\\4211\\\\Interim submission./dataport-export_gas_oct2015-mar2016.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "import timeit\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "# start_time = timeit.default_timer() * 1000\n",
    "currentPath = str(Path().resolve())\n",
    "# Put raw csv in same \n",
    "path = currentPath + \"./dataport-export_gas_oct2015-mar2016.csv\"\n",
    "\n",
    "# Read in raw data and find \n",
    "df = pd.read_csv(path)\n",
    "data_id = df['dataid'].nunique()\n",
    "print(\"Number of houses =\", data_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-10-01 00:14:44\n",
      "('Length of df:', 1584823)\n"
     ]
    }
   ],
   "source": [
    "# Sort data into order by id, if same id then by time\n",
    "df.sort_values([\"dataid\", \"localminute\"], ascending=[True, True], inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Cast localminute into proper time format for further processing\n",
    "df.localminute = df.localminute.str.slice(0,19)\n",
    "df.localminute = pd.to_datetime(df.localminute, infer_datetime_format = True, format = \"%Y/%m/%d %I:%M:%S %p\")\n",
    "print(df.localminute[0])\n",
    "print(\"Length of df:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Length of bad_array:', 169130)\n",
      "('Length of to_drop:', 1257748)\n"
     ]
    }
   ],
   "source": [
    "# Types of malfunction\n",
    "# 1: data reported back when change in gas use is < 2 cubic foot\n",
    "# 2: data reported back new meter_value is smaller than old meter_value\n",
    "# 3: data reported back is >2, but time > 15s, threshold value for reporting malfunction will be 7 cubic foot/hr \n",
    "# (US household average daily usage = 168 cubic foot)\n",
    "# 4: However, continuous report of same reading from same id over 12hrs,\n",
    "# the first reading after 12hrs will be treated as good reading,\n",
    "# as the gas company will probably want to know whether is a meter malfunctioning\n",
    "# even though its reading did not change for a period of time\n",
    "\n",
    "# Progress bar cuz I always feel it is not working\n",
    "# f = FloatProgress(min=0, max=(len(df['dataid']) - 1))\n",
    "# display(f)\n",
    "\n",
    "# Create empty df and array for more efficient removal of item in df\n",
    "malfunction = pd.DataFrame(columns = [\"localminute\", \"dataid\", \"meter_value\"])\n",
    "bad_array = []\n",
    "to_drop = []\n",
    "\n",
    "prev_good = True\n",
    "_id = None\n",
    "_12hr = None\n",
    "# Sort malfunction with time period label\n",
    "# Assumption: the first data point is always correct as the 2nd pt is wrt to it, 3rd wrt to 2nd....etc\n",
    "# Append malfunction period and data to respective array\n",
    "for row in df.itertuples():\n",
    "    if(_id is None or _id != row.dataid):\n",
    "        prev_good = True\n",
    "        _id = row.dataid\n",
    "    # for estimating time take to process actual data\n",
    "    if (row.Index == 0):\n",
    "        continue\n",
    "    #1 and #2\n",
    "    if (((row.meter_value <= df.meter_value[row.Index-1])\n",
    "         and (row.dataid == df.dataid[row.Index-1])) \n",
    "        or ((row.localminute != df.localminute[row.Index-1]) \n",
    "            and (row.dataid == df.dataid[row.Index-1]) \n",
    "            and ((row.meter_value - df.meter_value[row.Index-1]) < 2))):\n",
    "        if(prev_good == True):\n",
    "            bad_array.append([_id, row.localminute, row.localminute])\n",
    "            to_drop.append(row.Index)\n",
    "            #4, update 12hr pt\n",
    "            _12hr = row.localminute\n",
    "        else:\n",
    "            #4  \n",
    "            if ((row.localminute - _12hr) >= pd.to_timedelta(\"12:00:00\")):\n",
    "                prev_good = True\n",
    "                continue\n",
    "            else:\n",
    "                bad_array[-1][2] = row.localminute\n",
    "                to_drop.append(row.Index)\n",
    "        prev_good = False\n",
    "    else:\n",
    "        prev_good = True\n",
    "        #3\n",
    "        if(((row.meter_value - df.meter_value[row.Index-1]) > 2) \n",
    "           and ((((row.localminute - df.localminute[row.Index-1]) / timedelta(hours = 1)) * 7) \n",
    "                > (row.meter_value - df.meter_value[row.Index - 1]))):\n",
    "            bad_array[-1][2] = row.localminute\n",
    "print(\"Length of bad_array:\", len(bad_array))\n",
    "print(\"Length of to_drop:\", len(to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of df: 327075\n",
      "length of malfunction: 169130\n"
     ]
    }
   ],
   "source": [
    "#Remove malfunction data to produce clean df\n",
    "df.drop(index = to_drop, inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# # Cast to int64\n",
    "# malfunction['dataid'] = malfunction['dataid'].astype(np.int64)\n",
    "# malfunction['meter_value'] = malfunction['meter_value'].astype(np.int64)\n",
    "\n",
    "# # Merge 2 df to compare diff, if one of the label value is different,\n",
    "# # value in _merge label will be different hence being able to differeniate the difference\n",
    "# df = pd.merge(df, malfunction, on=['localminute', 'dataid', 'meter_value'], how='outer', indicator=True)\\\n",
    "# .query(\"_merge != 'both'\")\\\n",
    "# .drop(['_merge'], axis=1)\\\n",
    "# .reset_index(drop=True)\n",
    "\n",
    "# Convert bad_array into df and transform it for shape to be correct\n",
    "malfunction = pd.DataFrame(bad_array, columns = ['dataid', 'start_time', 'end_time'])\n",
    "malfunction.T\n",
    "\n",
    "# Save part1 result into csv for easier access in part2\n",
    "df.to_csv('./clean_df.csv', index = False)\n",
    "malfunction.to_csv('./malfunction.csv', index = False)\n",
    "\n",
    "# pd.set_option('display.max_rows', 10000)\n",
    "\n",
    "# elapsed = timeit.default_timer() * 1000 - start_time\n",
    "# print(\"total: %ds\" %(elapsed/1000)) if ((elapsed > 5000) == True) else print(\"total: %dms\" %elapsed)\n",
    "print(\"length of df:\", len(df))\n",
    "print(\"length of malfunction:\", len(malfunction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1.2</h1>\n",
    "In 1.2 we are supposed to generate hourly readings for different ids. The hourly readins will starts from the fisrt availble data point and end at the last available data point of the particular id, since we can hardly deduce the data point prior to the first point and after the last point. \n",
    "\n",
    "There are three main cases that we need to handle: \n",
    "1. A few (more than 1) consecutive data are within the same hour: in this case, we keep the last reading as the hourly reading because the last reading reflects the highest consumption of the hour\n",
    "2. The two consecutive datas are consecutive in time as well: in this case, we can savely keep the prior reading of the pair as the hourly reading.\n",
    "3. The two consecutive datas are more than one hour apart: in this case, there is missing data in between the two points and we need to fill in. This case can be furthure separated to 3 sub-cases:\n",
    "    3.1--The two consecutive data have identical meter_value: this means that there is no increase in value throughout the preiod, hence the missing hours have reading same as the closet available data.\n",
    "    3.2--The two consecutive data have different value and the value is less than 4: this means that there is increase during the period, but this increase is very likely to occur in the last hour, hence we will let missing hours' reading equal to their previous closest available reading.\n",
    "    3.3--The two consecutive data have different value and the value is more than 4: in this case, there is high possibility that in some hours, there is consumption but the meter fails to report. Hence, we will evenly distribute the difference among the missing hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "import datetime as dt \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327075"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import result from 1.1\n",
    "\n",
    "gas_data = pd.read_csv(\"clean_df.csv\")\n",
    "len(gas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing data for easier computation in the following: change datatype of 'localminute' to datetime\n",
    "#change datatype of 'meter_value' to float\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "gas_data.localminute = gas_data.localminute.str.slice(0,19)\n",
    "gas_data.localminute = pd.to_datetime(gas_data.localminute, infer_datetime_format = True, \n",
    "                                      format = \"%Y/%m/%d %I:%M:%S %p\");\n",
    "gas_data.localminute = gas_data.localminute.map(lambda x:x.replace(minute=0, second=0));\n",
    "gas_data['meter_value']=gas_data['meter_value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind=0;\n",
    "#creating a constant timedelta object with value =1 hr for comparison\n",
    "_hr=dt.timedelta(hours=1);\n",
    "temp_gas_hr=pd.DataFrame(columns=gas_data.columns);\n",
    "temp_gas_hr=gas_data;\n",
    "id_list=gas_data['dataid'].unique();\n",
    "#creating an id list for itertaion since only comparison of reading within id is meanningful\n",
    "\n",
    "#creating dummy list for appending later\n",
    "missing_lm=[];\n",
    "missing_id=[];\n",
    "missing_val=[];\n",
    "\n",
    "for _id in id_list:\n",
    "    \n",
    "    #generate hourly readings according to dataid\n",
    "    temp_gas_data=gas_data[gas_data['dataid']==_id];\n",
    "    temp_gas_data.reset_index(drop=True,inplace=True);\n",
    "    \n",
    "    for  row in temp_gas_data.itertuples():\n",
    "        if(row.Index==0):\n",
    "            prev_row=pd.Series(data=[row.localminute,row.dataid,row.meter_value]\n",
    "                               ,index=['localminute','dataid','meter_value']);\n",
    "            #unable to predict datapoints before the first available datapoint and afer the last datapoint\n",
    "            #hence the hourly reading starts from the first available local minute and end at the last one.\n",
    "        else:\n",
    "            \n",
    "            time_diff=row.localminute-prev_row.localminute;\n",
    "            #determine if there is any missing data before two consecutive available data\n",
    "            #if there is the difference in hour of two consecutive available data is large than one\n",
    "            #there is missing data\n",
    "            \n",
    "            if(time_diff>_hr):\n",
    "                time_diff=int(time_diff.total_seconds()/3600);\n",
    "                #determine how many datapoints are missing\n",
    "                \n",
    "                for j in range (1,time_diff):\n",
    "                    if ((row.meter_value-prev_row.meter_value)>4):\n",
    "                        #if the difference between two available datepoint is too large,\n",
    "                        #this means that there is missing distinct datapoints in between.\n",
    "                        acc_reading=float((row.meter_value-prev_row.meter_value)/time_diff);\n",
    "                    else:\n",
    "                        #if the difference is not large, the missing datapoint has value \n",
    "                        #equals to the previous datapoint\n",
    "                        acc_reading=0;\n",
    "                    time_change=dt.timedelta(hours=j);\n",
    "                    new_time=prev_row.localminute+time_change;\n",
    "                    missing_lm.append(new_time);\n",
    "                    missing_id.append(_id);\n",
    "                    missing_val.append(float(prev_row.meter_value+acc_reading*j));\n",
    "\n",
    "        prev_row=pd.Series(data=[row.localminute,row.dataid,row.meter_value]\n",
    "                               ,index=['localminute','dataid','meter_value']);\n",
    "       #make a copy the current available data for comparison in next iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concatenate the missing data with original data\n",
    "missing_data={'localminute':missing_lm,'dataid':missing_id,'meter_value':missing_val};\n",
    "missing_data=pd.DataFrame(missing_data,columns=gas_data.columns);\n",
    "temp_gas_hr=pd.concat([gas_data,missing_data]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 164s\n"
     ]
    }
   ],
   "source": [
    "temp_gas_hr=temp_gas_hr.sort_values(by=['dataid','localminute']);\n",
    "temp_gas_hr.drop_duplicates(['localminute','dataid'],keep='last',inplace=True); \n",
    "\n",
    "#for the same hour, if there is multiple readings, keep the highest value\n",
    "temp_gas_hr['meter_value']=temp_gas_hr['meter_value'].astype(int)\n",
    "print(\"total: %ds\" %(timeit.default_timer() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_gas_hr.to_csv('hourly_readings_final.csv',index=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color = \"red\">Run this cell only if generating graphs</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gas_hr=temp_gas_hr;\n",
    "# gas_hr=gas_hr[(gas_hr['localminute'].dt.year==2016)&(gas_hr['localminute'].dt.month==1)]\n",
    "# mon=2;\n",
    "# for _id in id_list:\n",
    "#     mon=2;\n",
    "#     temp_id_hr=gas_hr[gas_hr['dataid']==_id];\n",
    "#     while(len(temp_id_hr)==0):\n",
    "#         temp_id_hr=temp_gas_hr[(temp_gas_hr['localminute'].dt.year==2016)&(temp_gas_hr['localminute'].dt.month==mon)\n",
    "#                               &(temp_gas_hr['dataid']==_id)];\n",
    "#         mon=mon+1;\n",
    "#     temp_id_hr=temp_id_hr.reset_index(drop=True);\n",
    "#     t='Hourly reading of '+str(_id)+' of '+str(temp_id_hr.at[0,'localminute'].month)+' in '+str(temp_id_hr.at[0,'localminute'].year);\n",
    "#     fig=plt.figure(figsize=(15,15));\n",
    "#     plt.plot(temp_id_hr.localminute,temp_id_hr.meter_value);\n",
    "#     plt.xlabel('date hr/(hr)');\n",
    "#     plt.ylabel('meter_value'),\n",
    "#     plt.title(t,loc='center');\n",
    "#     plt.grid();\n",
    "#     t_fig=t+'.png';\n",
    "#     fig.savefig(t_fig);\n",
    "#     plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1.3</h1>\n",
    "In 1.3 we are required to compute the correlations between the gas consumption of every two households, and for each household, find out 5 other households with the highest correlation in gas consumption.\n",
    "\n",
    "This part of the code reads the <b><i>hourly_readings_final</i></b>.csv file which is generated in the previous section (1.2) and outputs <b><i>top_correlated_households</i></b>.csv. The correlation is calculated is based on the hourly readings for each household, as this method gives the same result as calculating based on hourly gas consumption for each household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt \n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.3\n",
    "hourly_reading = pd.read_csv('./hourly_readings_final.csv')\n",
    "\n",
    "# get the list of dataids\n",
    "idList = hourly_reading['dataid'].drop_duplicates(keep = 'first')\n",
    "idList = idList.reset_index(drop = True)\n",
    "\n",
    "len(idList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the gas meter readings based on data id\n",
    "# create a dataframe with only meter readings\n",
    "# the columns are all the different data ids\n",
    "rd = pd.DataFrame(columns = idList)\n",
    "for i in range(0, len(idList)):\n",
    "    rd[idList[i]] = hourly_reading.loc[hourly_reading.dataid == idList[i]]['meter_value'].reset_index(drop = True)\n",
    "\n",
    "# compute the correlation matrix of the new dataframe\n",
    "corr = rd.corr()\n",
    "\n",
    "# remove the 1s in diagonal\n",
    "# as the correlation between a column and itself is always 1\n",
    "corr -= np.eye(corr.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new dataframe with random values inserted\n",
    "top_corr = pd.DataFrame(np.random.randint(low = 0.0, high = 10.0, size = (5 * len(idList), 3)), columns = ['HH1', 'HH2', 'corr']).reset_index(drop = True)\n",
    "\n",
    "# set 'corr' column data type as float64\n",
    "# since correlation value is a floating point between 0 and 1\n",
    "top_corr['corr'] = top_corr['corr'].astype(float)\n",
    "\n",
    "# loop through all data ids in the correlation matrix\n",
    "# for each data id, find out the 5 data ids with the highest correlation values\n",
    "# collate all results in the top_corr dataframe\n",
    "count = 0\n",
    "for i in corr.columns[:]:\n",
    "    temp_corr = corr.nlargest(5, i)\n",
    "    for j in range (0, 5):\n",
    "        top_corr['HH1'].iloc[count] = i\n",
    "        top_corr['HH2'].iloc[count] = temp_corr.index[j]\n",
    "        top_corr['corr'].iloc[count] = temp_corr[i].iloc[j]\n",
    "        count += 1\n",
    "\n",
    "# export top_corr dataframe as csv file\n",
    "top_corr.to_csv('top_correlated_households.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
