{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1.1 </h1>\n",
    "<body>\n",
    "<font size = \"2\" color = \"red\">->Put raw csv in same path as ipynb for smooth running of code </font><br><br>\n",
    "\n",
    "<font size = \"3\">This part of the code finds the number of houses in to whole dataset,<br>\n",
    "outputs <b><i>clean_df</i></b> csv for further processing,<br>\n",
    "marks out the time period of all malfunction meter</font><br><br>\n",
    "\n",
    "In 1.1 we are supposed to generate a collections malfunction. We will first need to define what data points are counted as<br> malfunction, then we need to extract them. We have defined malfunction to be of these types:<br><br>\n",
    " 1: data reported back when change in gas use is < 2 cubic foot<br>\n",
    " 2: data reported back new meter_value is smaller than old meter_value<br>\n",
    " 3: data reported back is >2, but time > 15s, threshold value for reporting malfunction will be 7 cubic foot/hr<br> \n",
    " (US household average daily usage = 168 cubic foot)<br>\n",
    " If report between 2 reading is > 168, the extra high reading will be treated as malfunction and dropped away<br>\n",
    " 4: However, continuous report of same reading from same id over 12hrs,<br>\n",
    " the first reading after 12hrs will be treated as good reading,<br>\n",
    " as the gas company will probably want to know whether is a meter malfunctioning<br>\n",
    " even though its reading did not change for a period of time<br><br>\n",
    "\n",
    "And these are the main problems that we encountered,<br>\n",
    "- Finding efficient method to loop through the whole dataset, extracting what we want<br>\n",
    "- Looping through the dataset, removing what we don't want.<br>\n",
    "\n",
    "</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of houses = 157\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "import timeit\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "# start_time = timeit.default_timer() * 1000\n",
    "currentPath = str(Path().resolve())\n",
    "# Put raw csv in same \n",
    "path = currentPath + \"./dataport-export_gas_oct2015-mar2016.csv\"\n",
    "\n",
    "# Read in raw data and find \n",
    "df = pd.read_csv(path)\n",
    "data_id = df['dataid'].nunique()\n",
    "print(\"Number of houses =\", data_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-10-01 00:14:44\n",
      "Length of df: 1584823\n"
     ]
    }
   ],
   "source": [
    "# Sort data into order by id, if same id then by time\n",
    "df.sort_values([\"dataid\", \"localminute\"], ascending=[True, True], inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Cast localminute into proper time format for further processing\n",
    "df.localminute = df.localminute.str.slice(0,19)\n",
    "df.localminute = pd.to_datetime(df.localminute, infer_datetime_format = True, format = \"%Y/%m/%d %I:%M:%S %p\")\n",
    "print(df.localminute[0])\n",
    "print(\"Length of df:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of bad_array: 167948\n",
      "Length of to_drop: 1258484\n"
     ]
    }
   ],
   "source": [
    "# Types of malfunction\n",
    "# 1: data reported back when change in gas use is < 2 cubic foot\n",
    "# 2: data reported back new meter_value is smaller than old meter_value\n",
    "# 3: data reported back is >2, but time > 15s, threshold value for reporting malfunction will be 7 cubic foot/hr \n",
    "# (US household average daily usage = 168 cubic foot) https://www.aga.org/sites/default/files/aga_2961_2015_aga_playbook_final_0.pdf\n",
    "# If report between 2 reading is > 168, the extra high reading will be treated as malfunction and dropped away\n",
    "# 4: However, continuous report of same reading from same id over 12hrs,\n",
    "# the first reading after 12hrs will be treated as good reading,\n",
    "# as the gas company will probably want to know whether is a meter malfunctioning\n",
    "# even though its reading did not change for a period of time\n",
    "\n",
    "# Progress bar cuz I always feel it is not working\n",
    "# f = FloatProgress(min=0, max=(len(df['dataid']) - 1))\n",
    "# display(f)\n",
    "\n",
    "# Create empty df and array for more efficient removal of item in df\n",
    "malfunction = pd.DataFrame(columns = [\"localminute\", \"dataid\", \"meter_value\"])\n",
    "bad_array = []\n",
    "to_drop = []\n",
    "\n",
    "prev_good = True\n",
    "_id = None\n",
    "_12hr = None\n",
    "# Sort malfunction with time period label\n",
    "# Assumption: the first data point is always correct as the 2nd pt is wrt to it, 3rd wrt to 2nd....etc\n",
    "# Append malfunction period and data to respective array\n",
    "for row in df.itertuples():\n",
    "    if(_id is None or _id != row.dataid):\n",
    "        prev_good = True\n",
    "        _id = row.dataid\n",
    "    # for estimating time take to process actual data\n",
    "    if (row.Index == 0):\n",
    "        continue\n",
    "    #1 and #2\n",
    "    if (((row.meter_value <= df.meter_value[row.Index-1])\n",
    "         and (row.dataid == df.dataid[row.Index-1])) \n",
    "        or ((row.localminute != df.localminute[row.Index-1]) \n",
    "            and (row.dataid == df.dataid[row.Index-1]) \n",
    "            and ((row.meter_value - df.meter_value[row.Index-1]) < 2))):\n",
    "        if(prev_good == True):\n",
    "            bad_array.append([_id, row.localminute, row.localminute])\n",
    "            to_drop.append(row.Index)\n",
    "            #4, update 12hr pt\n",
    "            _12hr = row.localminute\n",
    "        else:\n",
    "            #4  \n",
    "            if ((row.localminute - _12hr) >= pd.to_timedelta(\"12:00:00\")):\n",
    "                prev_good = True\n",
    "                continue\n",
    "            else:                \n",
    "                if ((df.meter_value[row.Index - 1] - row.meter_value) < 168):\n",
    "                    to_drop.append(row.Index)\n",
    "                    bad_array[-1][2] = row.localminute                    \n",
    "        prev_good = False\n",
    "    else:\n",
    "        #3\n",
    "        if ((row.meter_value - df.meter_value[row.Index - 1]) < 168):\n",
    "            if(((row.meter_value - df.meter_value[row.Index-1]) > 2) \n",
    "               and ((((row.localminute - df.localminute[row.Index-1]) / timedelta(hours = 1)) * 7) \n",
    "                    > (row.meter_value - df.meter_value[row.Index - 1]))):\n",
    "                bad_array[-1][2] = row.localminute\n",
    "            prev_good = True\n",
    "        else:\n",
    "            to_drop.append(row.Index)\n",
    "            prev_good = False\n",
    "            bad_array[-1][2] = row.localminute\n",
    "            \n",
    "print(\"Length of bad_array:\", len(bad_array))\n",
    "print(\"Length of to_drop:\", len(to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of df: 326339\n",
      "length of malfunction: 167948\n"
     ]
    }
   ],
   "source": [
    "#Remove malfunction data to produce clean df\n",
    "df.drop(index = to_drop, inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# # Cast to int64\n",
    "# malfunction['dataid'] = malfunction['dataid'].astype(np.int64)\n",
    "# malfunction['meter_value'] = malfunction['meter_value'].astype(np.int64)\n",
    "\n",
    "# # Merge 2 df to compare diff, if one of the label value is different,\n",
    "# # value in _merge label will be different hence being able to differeniate the difference\n",
    "# df = pd.merge(df, malfunction, on=['localminute', 'dataid', 'meter_value'], how='outer', indicator=True)\\\n",
    "# .query(\"_merge != 'both'\")\\\n",
    "# .drop(['_merge'], axis=1)\\\n",
    "# .reset_index(drop=True)\n",
    "\n",
    "# Convert bad_array into df and transform it for shape to be correct\n",
    "malfunction = pd.DataFrame(bad_array, columns = ['dataid', 'start_time', 'end_time'])\n",
    "malfunction.T\n",
    "\n",
    "# Save part1 result into csv for easier access in part2\n",
    "df.to_csv('./clean_df.csv', index = False)\n",
    "malfunction.to_csv('./malfunction.csv', index = False)\n",
    "\n",
    "# pd.set_option('display.max_rows', 10000)\n",
    "\n",
    "# elapsed = timeit.default_timer() * 1000 - start_time\n",
    "# print(\"total: %ds\" %(elapsed/1000)) if ((elapsed > 5000) == True) else print(\"total: %dms\" %elapsed)\n",
    "print(\"length of df:\", len(df))\n",
    "print(\"length of malfunction:\", len(malfunction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1.2</h1>\n",
    "In 1.2 we are supposed to generate hourly readings for different ids. The hourly readins will starts from the fisrt availble data point and end at the last available data point of the particular id, since we can hardly deduce the data point prior to the first point and after the last point. \n",
    "\n",
    "There are three main cases that we need to handle: \n",
    "1. A few (more than 1) consecutive data are within the same hour: in this case, we keep the last reading as the hourly reading because the last reading reflects the highest consumption of the hour\n",
    "2. The two consecutive datas are consecutive in time as well: in this case, we can savely keep the prior reading of the pair as the hourly reading.\n",
    "3. The two consecutive datas are more than one hour apart: in this case, there is missing data in between the two points and we need to fill in. This case can be furthure separated to 3 sub-cases:\n",
    "    3.1--The two consecutive data have identical meter_value: this means that there is no increase in value throughout the preiod, hence the missing hours have reading same as the closet available data.\n",
    "    3.2--The two consecutive data have different value and the value is less than 4: this means that there is increase during the period, but this increase is very likely to occur in the last hour, hence we will let missing hours' reading equal to their previous closest available reading.\n",
    "    3.3--The two consecutive data have different value and the value is more than 4: in this case, there is high possibility that in some hours, there is consumption but the meter fails to report. Hence, we will evenly distribute the difference among the missing hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "import datetime as dt \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326339"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import result from 1.1\n",
    "\n",
    "gas_data = pd.read_csv(\"clean_df.csv\")\n",
    "len(gas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing data for easier computation in the following: change datatype of 'localminute' to datetime\n",
    "#change datatype of 'meter_value' to float\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "gas_data.localminute = gas_data.localminute.str.slice(0,19)\n",
    "gas_data.localminute = pd.to_datetime(gas_data.localminute, infer_datetime_format = True, \n",
    "                                      format = \"%Y/%m/%d %I:%M:%S %p\");\n",
    "gas_data.localminute = gas_data.localminute.map(lambda x:x.replace(minute=0, second=0));\n",
    "gas_data['meter_value']=gas_data['meter_value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=0;\n",
    "#creating a constant timedelta object with value =1 hr for comparison\n",
    "_hr=dt.timedelta(hours=1);\n",
    "temp_gas_hr=pd.DataFrame(columns=gas_data.columns);\n",
    "temp_gas_hr=gas_data;\n",
    "id_list=gas_data['dataid'].unique();\n",
    "#creating an id list for itertaion since only comparison of reading within id is meanningful\n",
    "\n",
    "#creating dummy list for appending later\n",
    "missing_lm=[];\n",
    "missing_id=[];\n",
    "missing_val=[];\n",
    "\n",
    "for _id in id_list:\n",
    "    \n",
    "    #generate hourly readings according to dataid\n",
    "    temp_gas_data=gas_data[gas_data['dataid']==_id];\n",
    "    temp_gas_data.reset_index(drop=True,inplace=True);\n",
    "    \n",
    "    for  row in temp_gas_data.itertuples():\n",
    "        if(row.Index==0):\n",
    "            prev_row=pd.Series(data=[row.localminute,row.dataid,row.meter_value]\n",
    "                               ,index=['localminute','dataid','meter_value']);\n",
    "            #unable to predict datapoints before the first available datapoint and afer the last datapoint\n",
    "            #hence the hourly reading starts from the first available local minute and end at the last one.\n",
    "        else:\n",
    "            \n",
    "            time_diff=row.localminute-prev_row.localminute;\n",
    "            #determine if there is any missing data before two consecutive available data\n",
    "            #if there is the difference in hour of two consecutive available data is large than one\n",
    "            #there is missing data\n",
    "            \n",
    "            if(time_diff>_hr):\n",
    "                time_diff=int(time_diff.total_seconds()/3600);\n",
    "                #determine how many datapoints are missing\n",
    "                \n",
    "                for j in range (1,time_diff):\n",
    "                    if ((row.meter_value-prev_row.meter_value)>4):\n",
    "                        #if the difference between two available datepoint is too large,\n",
    "                        #this means that there is missing distinct datapoints in between.\n",
    "                        acc_reading=float((row.meter_value-prev_row.meter_value)/time_diff);\n",
    "                    else:\n",
    "                        #if the difference is not large, the missing datapoint has value \n",
    "                        #equals to the previous datapoint\n",
    "                        acc_reading=0;\n",
    "                    time_change=dt.timedelta(hours=j);\n",
    "                    new_time=prev_row.localminute+time_change;\n",
    "                    missing_lm.append(new_time);\n",
    "                    missing_id.append(_id);\n",
    "                    missing_val.append(float(prev_row.meter_value+acc_reading*j));\n",
    "\n",
    "        prev_row=pd.Series(data=[row.localminute,row.dataid,row.meter_value]\n",
    "                               ,index=['localminute','dataid','meter_value']);\n",
    "       #make a copy the current available data for comparison in next iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the missing data with original data\n",
    "missing_data={'localminute':missing_lm,'dataid':missing_id,'meter_value':missing_val};\n",
    "missing_data=pd.DataFrame(missing_data,columns=gas_data.columns);\n",
    "temp_gas_hr=pd.concat([gas_data,missing_data]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 171s\n"
     ]
    }
   ],
   "source": [
    "temp_gas_hr=temp_gas_hr.sort_values(by=['dataid','localminute']);\n",
    "temp_gas_hr.drop_duplicates(['localminute','dataid'],keep='last',inplace=True); \n",
    "\n",
    "#for the same hour, if there is multiple readings, keep the highest value\n",
    "temp_gas_hr['meter_value']=temp_gas_hr['meter_value'].astype(int)\n",
    "print(\"total: %ds\" %(timeit.default_timer() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_gas_hr.to_csv('hourly_readings_final.csv',index=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color = \"red\">Run this cell only if generating graphs</font></h1>\n",
    "<h5>may take 3 hours</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gas_hr=temp_gas_hr;\n",
    "# gas_hr=gas_hr[(gas_hr['localminute'].dt.year==2016)&(gas_hr['localminute'].dt.month==1)]\n",
    "# mon=2;\n",
    "# for _id in id_list:\n",
    "#     mon=2;\n",
    "#     temp_id_hr=gas_hr[gas_hr['dataid']==_id];\n",
    "#     while(len(temp_id_hr)==0):\n",
    "#         temp_id_hr=temp_gas_hr[(temp_gas_hr['localminute'].dt.year==2016)&(temp_gas_hr['localminute'].dt.month==mon)\n",
    "#                               &(temp_gas_hr['dataid']==_id)];\n",
    "#         mon=mon+1;\n",
    "#     temp_id_hr=temp_id_hr.reset_index(drop=True);\n",
    "#     t='Hourly reading of '+str(_id)+' of '+str(temp_id_hr.at[0,'localminute'].month)+' in '+str(temp_id_hr.at[0,'localminute'].year);\n",
    "#     fig=plt.figure(figsize=(15,15));\n",
    "#     plt.plot(temp_id_hr.localminute,temp_id_hr.meter_value);\n",
    "#     plt.xlabel('date hr/(hr)');\n",
    "#     plt.ylabel('meter_value'),\n",
    "#     plt.title(t,loc='center');\n",
    "#     plt.grid();\n",
    "#     t_fig=t+'.png';\n",
    "#     fig.savefig(t_fig);\n",
    "#     plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1.3</h1>\n",
    "In 1.3 we are required to compute the correlations between the gas consumption of every two households, and for each household, find out 5 other households with the highest correlation in gas consumption.\n",
    "\n",
    "This part of the code reads the <b><i>hourly_readings_final</i></b>.csv file which is generated in the previous section (1.2) and outputs <b><i>top_correlated_households</i></b>.csv. The main problem faced in this section is how the correlation should be computed. The initial idea was to calculate only based on hourly readings computed in 1.2 without considering time. However, many of the households do not have complete data, which means in some month, these households do not have any meter readings. Thus simply computing correlation based on hourly readings is not sufficient.\n",
    "\n",
    "The method used for 1.3 is to compute the average gas consumption for each household in each hour. The hourly consumption is computed first by comparing the readings between adjacent readings for the same household. After getting the hourly consumption, the mean of all readings in each hour is computed, as it is reasonable to say that by including more data points, the average can be more representative compared to taking a few data points. For each household, there are 24 data points for average hourly consumption, representing the 24 hours. Correlation matrix can be computed from the average hourly consumption, and the top 5 households with highest correlation can be found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt \n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3\n",
    "# import result obtained from 1.2\n",
    "hourly_reading = pd.read_csv('./hourly_readings_final.csv')\n",
    "\n",
    "# get the list of dataids\n",
    "idList = hourly_reading['dataid'].drop_duplicates(keep = 'first')\n",
    "idList = idList.reset_index(drop = True)\n",
    "\n",
    "# split the column 'localminute' into 'day' and 'time'\n",
    "new_time = hourly_reading['localminute'].str.split(\" \", n = 1, expand = True)\n",
    "hourly_reading['day'] = new_time[0]\n",
    "hourly_reading['time'] = new_time[1]\n",
    "hourly_reading.drop(columns = ['localminute'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out all readings from household 6101 to show the\n",
    "# insufficient information available for this household\n",
    "print(hourly_reading.loc[hourly_reading.dataid == 6101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add the 'diffs' column which shows the hourly consumption by each household\n",
    "# the function diff() computes the difference between the current data point and the previous data point\n",
    "# therefore the data points at the boundary (data id changes) are wrong and will be changed to NaN\n",
    "hourly_reading['diffs'] = hourly_reading['meter_value'].diff()\n",
    "mask = (hourly_reading.dataid != hourly_reading.dataid.shift(1))\n",
    "hourly_reading.loc[mask, 'diffs'] = np.nan\n",
    "\n",
    "# compute the average hourly consumption by each household\n",
    "# save all the values in the average_hourly_consumption dataframe created\n",
    "average_hourly_consumption = pd.DataFrame(np.random.randint(low = 0.0, high = 10.0, size = (24, len(idList))), columns = idList)\n",
    "for col in idList:\n",
    "    for hr in range (0, 23):\n",
    "#         set the criteria for selecting the data points\n",
    "        if hr < 10:\n",
    "            criterion = hourly_reading['time'].map(lambda x: (x[0] ==  '0') & (x[1] == str(hr)))\n",
    "        if hr >= 10:\n",
    "            criterion = hourly_reading['time'].map(lambda x: (x[0] == str(hr)[0]) & (x[1] ==  str(hr)[1]))\n",
    "#         compute the mean of hourly consumption for each household and insert into average_hourly_consumption dataframe\n",
    "        average_hourly_consumption.loc[hr, col] = hourly_reading.loc[(hourly_reading.dataid == col) & criterion]['diffs'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlation matrix of the new dataframe\n",
    "corr = average_hourly_consumption.corr()\n",
    "\n",
    "# remove the 1s in diagonal\n",
    "# as the correlation between a column and itself is always 1\n",
    "corr -= np.eye(corr.shape[0])\n",
    "\n",
    "# create a new dataframe with random values inserted\n",
    "top_corr = pd.DataFrame(np.random.randint(low = 0.0, high = 10.0, size = (5 * len(idList), 3)), columns = ['HH1', 'HH2', 'corr']).reset_index(drop = True)\n",
    "\n",
    "# set 'corr' column data type as float64\n",
    "# since correlation value is a floating point between 0 and 1\n",
    "top_corr['corr'] = top_corr['corr'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e0631b01c694>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtop_corr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'HH1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mtop_corr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'HH2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_corr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mtop_corr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'corr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_corr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2068\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2069\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# loop through all data ids in the correlation matrix\n",
    "# for each data id, find out the 5 data ids with the highest correlation values\n",
    "# collate all results in the top_corr dataframe\n",
    "count = 0\n",
    "for i in corr.columns[:]:\n",
    "    temp_corr = corr.nlargest(5, i)\n",
    "    \n",
    "#     since household 6101 has very limited information (only one available datum)\n",
    "#     it is discarded in the calculation of correlations between households\n",
    "    if i != 6101:\n",
    "        for j in range (0, 5):\n",
    "            top_corr.loc[count, 'HH1'] = i\n",
    "            top_corr.loc[count, 'HH2'] = temp_corr.index[j]\n",
    "            top_corr.loc[count, 'corr'] = temp_corr[i].iloc[j]\n",
    "            count += 1\n",
    "\n",
    "# export top_corr dataframe as csv file\n",
    "top_corr.to_csv('top_correlated_households.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1.4</h1>\n",
    "\n",
    "In our proposal, we would like to analyze the gas consumption data further by applying clustering method to measure the similarity between the households, and group them based on energy usage. The goal in clustering time‐series data is to understand user behavior by organizing the data into homogeneous groups, maximizing the similarity and dissimilarity within and between groups, respectively.\n",
    "\n",
    "Clustering is a collection of same group similar or data objects or in other groups. Its also finding the dissimilar to the data objects in other groups.In cluster analysis, the main objective is to find similarities between data objects with the help of specific characteristics found in the data and grouping these similar data objects into clusters. In our project, the following features and their respective standard deviation will be examined in the following clustering discussion.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Features</th>\n",
    "<th>Discription</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Total consumption</td>\n",
    "<td>Total consumed power during period of the study</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Morning consumtption</td>\n",
    "<td>The consumption observed from 06:00 - 09:00</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Noon consumtption</td>\n",
    "<td>The consumption observed from 11:00 - 14:00</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Night consumtption</td>\n",
    "<td>The consumption observed from 17:00 - 20:00</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential application areas for work using this form of cluster analysis as more smart data becomes routinely available include\n",
    "1. Application of cluster results for segment-specific rate design: the economic benefits of time-variable natural gas rates consist in the potential for utilities to improve price discrimination and to facilitate the reduction of peak loads. A complete design of gas scheme would encompass number of different time zones and starting time of each period. All these could be done through clustering analysis. In addition, by influencing the demand side, segment-specific natural gas rates can improve the energy system’s efficiency.\n",
    "\n",
    "2. Application of cluster results for leak detection activities based on detecting pattern changes (deviation from cluster centroids/ distributions); data-driven models of demand could also help identify atypical customers or unusual changes in consumption. \n",
    "\n",
    "3. Application of cluster results for filling missing data for audits/ regulatory purposes using cluster centroids. Typical usage perhaps allowS volumetric usage and flow profiles to be estimated for unmetered customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "import timeit\n",
    "import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import HTML\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path().resolve().parent)+'\\csv'\n",
    "df = pd.read_csv(path+\"/hourly_consumption.csv\")\n",
    "\n",
    "df.localminute = df.localminute.str.slice(0,19)\n",
    "df.localminute = pd.to_datetime(df.localminute, infer_datetime_format = True, \n",
    "                                      format = \"%Y/%m/%d %I:%M:%S %p\");\n",
    "headers = [\"dataid\", \"totoalConsumption\", \"morningMean\", \"morningSTD\", \"noonMean\",\n",
    "          \"noonSTD\", \"nightMean\", \"nightSTD\"]\n",
    "df.localminute = df.localminute.map(lambda x:x.replace(minute=0, second=0));\n",
    "currentID = df.iloc[0]['dataid']\n",
    "baseTime = df.iloc[0]['localminute'] \n",
    "featuresList = []\n",
    "morningConsumption = []\n",
    "noonConsumption = []\n",
    "nightConsumption = []\n",
    "totoalConsumption = 0\n",
    "currentMorning = 0\n",
    "currentNoon = 0\n",
    "currentNight = 0\n",
    "prevDay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction \n",
    "for row in df.itertuples():\n",
    "    time_diff = str(row.localminute - baseTime)\n",
    "    hrs_diff = int(time_diff.split(\" \")[2][:2])\n",
    "    day_diff = int((time_diff.split(\" \")[0]))\n",
    "    if (day_diff != prevDay):\n",
    "        morningConsumption.append(currentMorning)\n",
    "        noonConsumption.append(currentNoon)\n",
    "        nightConsumption.append(currentNight)\n",
    "        prevDay = day_diff\n",
    "        currentMorning = 0\n",
    "        currentNoon = 0\n",
    "        currentNight = 0\n",
    "        \n",
    "    if (currentID != row.dataid):\n",
    "        features = []\n",
    "        morningMean = np.mean(morningConsumption)\n",
    "        morningSTD = np.std(morningConsumption)\n",
    "        noonMean = np.mean(noonConsumption)\n",
    "        noonSTD = np.std(noonConsumption)\n",
    "        nightMean = np.mean(nightConsumption)\n",
    "        nightSTD = np.std(nightConsumption)\n",
    "        features.append(currentID)\n",
    "        features.append(totoalConsumption)\n",
    "        features.append(morningMean)\n",
    "        features.append(morningSTD)\n",
    "        features.append(noonMean)\n",
    "        features.append(noonSTD)\n",
    "        features.append(nightMean)\n",
    "        features.append(nightSTD)\n",
    "        featuresList.append(features)\n",
    "        \n",
    "        morningConsumption = []\n",
    "        noonConsumption = []\n",
    "        nightConsumption = []\n",
    "        totoalConsumption = 0\n",
    "        currentID = row.dataid\n",
    "    \n",
    "    totoalConsumption = totoalConsumption + int(row.meter_value)\n",
    "    if (hrs_diff >= 6 and hrs_diff < 9):\n",
    "        currentMorning = currentMorning + int(row.meter_value)\n",
    "    elif (hrs_diff >= 11 and hrs_diff < 14):\n",
    "        currentNoon = currentNoon + int(row.meter_value)\n",
    "    elif (hrs_diff >= 17 and hrs_diff < 20):\n",
    "        currentNight = currentNight + int(row.meter_value)\n",
    "\n",
    "morningConsumption.append(currentMorning)\n",
    "noonConsumption.append(currentNoon)\n",
    "nightConsumption.append(currentNight)\n",
    "morningMean = np.mean(morningConsumption)\n",
    "morningSTD = np.std(morningConsumption)\n",
    "noonMean = np.mean(noonConsumption)\n",
    "noonSTD = np.std(noonConsumption)\n",
    "nightMean = np.mean(nightConsumption)\n",
    "nightSTD = np.std(nightConsumption)\n",
    "features = []\n",
    "\n",
    "features.append(currentID)\n",
    "features.append(totoalConsumption)\n",
    "features.append(morningMean)\n",
    "features.append(morningSTD)\n",
    "features.append(noonMean)\n",
    "features.append(noonSTD)\n",
    "features.append(nightMean)\n",
    "features.append(nightSTD)\n",
    "featuresList.append(features)\n",
    "\n",
    "featureFrame = pd.DataFrame(featuresList)\n",
    "featureFrame.to_csv(path + '/reading_features.csv', header=headers, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-medoids clustering selects the most centrally located data points within clusters as cluster centers called medoids and returns the number of clustering as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+\"/reading_features.csv\")\n",
    "featureNames = [\"totoalConsumption\", \"morningMean\", \"noonMean\", \"nightMean\"]\n",
    "\n",
    "\n",
    "x = df[featureNames].values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3).fit(df_normalized)\n",
    "df.insert(loc=0, column=\"classification\", value=kmeans.labels_)\n",
    "df.to_csv(path + '/reading_features_classification.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array = df.values\n",
    "plt.scatter(df_array[:,2],df_array[:,3], c=kmeans.labels_, cmap='rainbow')  \n",
    "plt.xlabel('Totoal Consumption')\n",
    "plt.ylabel('Morning Average Consumption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array = df.values\n",
    "plt.scatter(df_array[:,2],df_array[:,5], c=kmeans.labels_, cmap='rainbow')  \n",
    "plt.xlabel('Totoal Consumption')\n",
    "plt.ylabel('Noon Average Consumption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array = df.values\n",
    "plt.scatter(df_array[:,2],df_array[:,7], c=kmeans.labels_, cmap='rainbow')  \n",
    "plt.xlabel('Totoal Consumption')\n",
    "plt.ylabel('Night Average Consumption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(df[featureNames])\n",
    "pca_2d = pca.transform(df[featureNames])\n",
    "for i in range(0, pca_2d.shape[0]):\n",
    "    if df.classification[i] == 0:\n",
    "        c1 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='r',marker='+')\n",
    "    elif df.classification[i] == 1:\n",
    "        c2 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='g',marker='o')\n",
    "    elif df.classification[i] == 2:\n",
    "        c3 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='b',marker='*')\n",
    "pl.legend([c1, c2, c3], ['A1', 'A2','A3'])\n",
    "pl.title('Gas consumption dataset with 3 clusters and knownoutcomes')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, data clustering is used to find similar groups having the same gas consumption patterns. Smart meter data clustering has been widely investigated toward consumers grouping and revealing their energy usage behavior which leads to more efficient tariff policy and tailored energy efficiency programs for specific users. More specific data analysis and comparison will be shown in section 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
