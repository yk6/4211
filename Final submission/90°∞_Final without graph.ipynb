{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 \n",
    "1.Can you explain why you may want to forecast the gas consumption in the future?\n",
    "\n",
    "The first reason why we want to predict is to detect gas leakage as soon as possible. By predicting the gas consumption in the future, when the actual consumption is far above (>50) than the prediction, it is very likely that there is gas leakage of the system. This is very important since the cost of gas leakage is very huge and we should detect this leakage as soon as possible and act on it. Hence, when there is a possible leakage, we should let the smart meter send a warning to both the gas company and the user to validate the situation.\n",
    "\n",
    "The second reason why we want to predict is that we can adjust the gas supply at particular period. By prediction, the gas company can anticipate what is period that is likely to experience high demand. With this information, the gas company can adjust their supply at particular period to fulfill the demand and to keep less waste at period when there is less demand.\n",
    "\n",
    "The third reason is that we can use the prediction to detect if the smart gas meter is malfunctioning. When the actual meter reading and the prediction behave very different, there is potential that the samrt meter is broken. Since malfunctionning of the meter might cause dispute in the final bill, the company should detect this and send in technician to fix it as soon as possible.\n",
    "\n",
    "2.Who would find this information valuable?\n",
    "\n",
    "Both the company and the customers will find it valuable\n",
    "\n",
    "3.What can you do if you have a good forecasting model?\n",
    "\n",
    "We can integrate with an IoT system for gas leakage detection and alarm and meter malfunction detection. The gas company can also use this prediction as a reference for gas generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 \n",
    "In this section, we are building a prediction system using linear regression. To fit the data to the model, there are two processes that have been done. \n",
    "\n",
    "Data Preproccessing:\n",
    "\n",
    "Firstly, the time series has been converted to numbers. This is done by setting the earliest time point of the whole data set as 0 and increment of 1 meaning 1 hour later than the earliest time point(time_min). In this way, we are able to deal with time-series. When prediction occurs, we need to convert the time number back to a time point by adding the time number with time_min. \n",
    "\n",
    "Secondly,in order to calculate consumption, we have initialize the first available reading of a particular id as the start point (consumption=0) and calculate consumption at particular time point by comparing the meter reading at that time with the reading at the start point. This is necessary as there are some id that has insufficient amount of data points and we want to replenish data of these id by comparing the id's consumption with other id's. If consumption is not extracted from original meter reading, it is not possible for us to have good replenishment for those data.\n",
    "\n",
    "To train the model, since we have sufficient datas, the train to test ratio=9:1.\n",
    "\n",
    "We only train for id with more than 10 data points. Since if the id has too little data, the prediction model would be meaningless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.svm import SVR\n",
    "import sklearn.model_selection as model_sel\n",
    "import datetime as dt \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_hr=pd.read_csv('./csv/hourly_readings_final.csv');\n",
    "#id 6101 is dropped because it only has one data and there is no prediction can be done on one data point\n",
    "id_list=gas_hr['dataid'].unique()\n",
    "for _id in id_list:\n",
    "    temp_data=gas_hr[gas_hr['dataid']==_id];\n",
    "    if(len(temp_data)<10):\n",
    "        gas_hr=gas_hr[gas_hr.dataid!=_id];\n",
    "\n",
    "#Converting time varible to integer number\n",
    "gas_hr.localminute = pd.to_datetime(gas_hr.localminute, infer_datetime_format = True,format = \"%Y/%m/%d %I:%M:%S %p\");\n",
    "min_time=min(gas_hr['localminute']);\n",
    "gas_hr.localminute=gas_hr.localminute.map(lambda x: (x-min_time).total_seconds()/3600.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list=gas_hr['dataid'].unique()\n",
    "len(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_gas_hr=pd.DataFrame();\n",
    "min_consump=[];\n",
    "\n",
    "#Converting meter readings to consumption of all ids\n",
    "for _id in id_list:\n",
    "    gas_temp=gas_hr[gas_hr['dataid']==_id];\n",
    "    min_consumption=min(gas_temp['meter_value']);\n",
    "    gas_temp['meter_value']=gas_temp.meter_value.map(lambda x: x-min_consumption);\n",
    "    min_consump.append(min_consumption);\n",
    "    normed_gas_hr=normed_gas_hr.append(gas_temp);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is meant to detect those id whose amount of data is deemed as insufficient. We have set the threshold point of amount of data to 2000 (2 months). We have extracted a list of ids and will subsititue more data points later.Since the subsitituded data is not the real data, it will only be used for training but not testing.\n",
    "\n",
    "(let say we are subsitituting data for id A)\n",
    "\n",
    "Firstly, we only use that that is within id A's data range. For example, if A only has data of Dec in 2015, we will only compare other id's data which is within Dec in 2015. The reason for this is that in our model, we assume strong relation between time and reading. Since time is assumed deterministric to actual reading, we can only consider data within the same period.\n",
    "\n",
    "Secondly, the meteric used to determine if the data of another id can be used is consumption. For different id, the reading of the start point might not be the same, even though the consumption might be the same. Hence, by comparing readings directly, it is very likely that the comparison is not accurate.\n",
    "\n",
    "Logic for subsititution of data:(let say we are subsitituting data for id A) 1. find out the time range of id A. 2. Extract data for all other ids within the same range. 3. loop through all other ids. If for id B, the available data has range that is narrower than id A, narrow id A's range to that range. Generate gas consumption data of the particular period. Divide B's consumption by A's and compare the results(div_result). A soft margin is set to allow for some discripency of consumption. Two point from id A and B are considered matched if div_result=1 or div_result = NaN(NaN is output of 0/0) and 1-soft margin<=div_result<=1+soft_margin. Compute number of points that are matched and see if this number exceeds x%(acceptance_lvl) of total number of data within the period. If yes, subsititute these data as id A's data. The smaller the soft margin and the higher the acceptance level, the less but more fitted data will be subed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_lack=[]\n",
    "length=[]\n",
    "thresh_length=2000;\n",
    "\n",
    "#Determine what id need to be subed\n",
    "for _id in id_list:\n",
    "    gas_hr_id=gas_hr[gas_hr['dataid']==_id];\n",
    "    length.append(len(gas_hr_id))\n",
    "for index, item in enumerate(length):\n",
    "    if((item<thresh_length) & (item>10)):\n",
    "        id_lack.append(id_list[index]);\n",
    "id_lack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_margin=0.2;\n",
    "acceptance_lvl=0.9;\n",
    "#Create a varibale to store the subed data\n",
    "normed_gas_hr_cp=pd.DataFrame(columns=['localminute','dataid','meter_value'])\n",
    "for _id in id_lack:\n",
    "    gas_lack_hr=normed_gas_hr[normed_gas_hr['dataid']==_id];\n",
    "    time_min=min(gas_lack_hr['localminute']);\n",
    "    time_max=max(gas_lack_hr['localminute']);\n",
    "    \n",
    "    #Extract id's who has data that is within time range we are looking for\n",
    "    gas_compensate=normed_gas_hr[(normed_gas_hr['localminute']>=time_min) & (normed_gas_hr['localminute']<=time_max) \n",
    "                         &(normed_gas_hr['dataid']!=_id)];\n",
    "    id_com=gas_compensate['dataid'].unique();\n",
    "    \n",
    "    #Loop through all candidate ids, searching for subsititution\n",
    "    for c_id in id_com:\n",
    "        #Find out the range of data for comparison and compute consumption based on readings\n",
    "        gas_cid=pd.DataFrame();\n",
    "        gas_cid=gas_compensate[gas_compensate['dataid']==c_id];\n",
    "        min_consumption=min(gas_cid['meter_value']);\n",
    "        gas_cid['meter_value']=gas_cid.meter_value.map(lambda x: x-min_consumption);\n",
    "        gas_cid.reset_index(drop=True,inplace=True);\n",
    "        \n",
    "        #If id B has a smaller range than id A\n",
    "        #Extract data from id A within the same range and compute for consumption\n",
    "        time_min_c=min(gas_cid['localminute']);\n",
    "        time_max_c=max(gas_cid['localminute']);\n",
    "        gas_lack_c=pd.DataFrame();\n",
    "        if((time_min_c!=time_min) | (time_max_c!=time_max)):\n",
    "            gas_lack_c=gas_lack_hr[(gas_lack_hr['localminute']>=time_min_c) & (gas_lack_hr['localminute']<=time_max_c)];\n",
    "            min_consumption=min(gas_lack_c['meter_value']);\n",
    "            gas_lack_c['meter_value']=gas_lack_c.meter_value.map(lambda x: x-min_consumption);\n",
    "        else:\n",
    "            gas_lack_c=gas_lack_hr;\n",
    "        gas_lack_c.reset_index(drop=True,inplace=True);\n",
    "        \n",
    "        #Compute difference between two data set by division\n",
    "        diff_percent=gas_cid['meter_value']/gas_lack_c['meter_value'];\n",
    "        #Find out the amount of matched data points\n",
    "        equal_num=len(diff_percent[(diff_percent<=(1+soft_margin))&\n",
    "                                   (diff_percent>=(1-soft_margin))])+len(diff_percent[np.isnan(diff_percent)]);\n",
    "        \n",
    "        #From the number of matched data points, determine if the data can be used as subsititution\n",
    "        if(equal_num>=acceptance_lvl*len(gas_cid)):\n",
    "            gas_cid.dataid=gas_cid.dataid.replace(c_id,_id);\n",
    "            if(time_min_c!=time_min):\n",
    "                temp_reading=gas_lack_hr[gas_lack_hr['localminute']==time_min_c]['meter_value'].values;\n",
    "                extra_reading=temp_reading[0];\n",
    "                gas_cid['meter_value']=gas_cid.meter_value.map(lambda x: x+extra_reading);\n",
    "            normed_gas_hr_cp=normed_gas_hr_cp.append(gas_cid);\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots to see what has been subed for the lacking ids\n",
    "\n",
    "# for _id in id_lack:\n",
    "#     temp=normed_gas_hr_cp[normed_gas_hr_cp['dataid']==_id]\n",
    "#     temp.reset_index(drop=True,inplace=True)\n",
    "#     fig=plt.figure(figsize=(5,5));\n",
    "#     plt.plot(temp.localminute,temp.meter_value,'r');\n",
    "#     plt.xlabel('date hr/(hr)');\n",
    "#     plt.ylabel('meter_value'),\n",
    "#     plt.grid();\n",
    "#     plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_array=[];\n",
    "score=[];\n",
    "decre_index=0;\n",
    "id_list1=[35]\n",
    "all_result=pd.DataFrame(columns=['localminute','dataid','actual_reading','predicted_reading']);\n",
    "\n",
    "for index,item in enumerate(id_list):\n",
    "    #Extract attributes and wanted values\n",
    "    X_temp=normed_gas_hr[normed_gas_hr['dataid']==item][['localminute']];\n",
    "    Y_temp=normed_gas_hr[normed_gas_hr['dataid']==item][['meter_value']];\n",
    "    #Split train and test case\n",
    "    (X_train, X_test, Y_train, Y_test)=model_sel.train_test_split(X_temp,Y_temp,test_size=0.1);\n",
    "    \n",
    "    #For lacking ids add in more training data\n",
    "    if(item in id_lack):\n",
    "        X_train=X_train.append(normed_gas_hr_cp[normed_gas_hr_cp['dataid']==item][['localminute']]);\n",
    "        Y_train=Y_train.append(normed_gas_hr_cp[normed_gas_hr_cp['dataid']==item][['meter_value']]);\n",
    "    \n",
    "    #only train for model has more than 10 data points, since the model will be more or less meaningless with \n",
    "    #very little amout of data\n",
    "    if(len(X_temp)<10):\n",
    "        P_array.append('not enough data');\n",
    "        score.append('not enough data');\n",
    "    else:\n",
    "        #Training using LR model\n",
    "        LR=lm.LinearRegression();\n",
    "        predictor=LR.fit(X_train,Y_train);\n",
    "        #Stor the trained predictor and the score of the prefictor for future use\n",
    "        P_array.append(predictor)\n",
    "        score.append(P_array[index].score(X_test,Y_test));\n",
    "        #Predict for test data for comparison\n",
    "        predicted=P_array[index].predict(X_test);\n",
    "        \n",
    "        #Convert back the time number to time point\n",
    "        #Convert back consumption to meter readings\n",
    "        X_test=X_test.localminute.map(lambda x: dt.timedelta(hours=x)+min_time);\n",
    "        Y_test=Y_test.meter_value.map(lambda x:x+min_consump[index]);\n",
    "        predicted=np.add(predicted,min_consump[index]);\n",
    "        result=pd.DataFrame(columns=['localminute','dataid','actual_reading','predicted_reading']);\n",
    "        result['localminute']=X_test;\n",
    "        result['actual_reading']=Y_test;\n",
    "        result['predicted_reading']=predicted;\n",
    "        result['dataid']=item;\n",
    "        result=result.sort_values(by='localminute');\n",
    "        #Store the predicted result for plots in the next section\n",
    "        all_result = all_result.append(result)\n",
    "        \n",
    "        #Plot of prediction vs actual reading against time\n",
    "        fig=plt.figure(figsize=(10,10));\n",
    "        t='Prediction & Actual reading vs time of '+str(item)\n",
    "        plt.plot(result.localminute,result.actual_reading,'r',label='Actual');\n",
    "        plt.plot(result.localminute,result.predicted_reading,'b',label='Predicted');\n",
    "        plt.legend()\n",
    "        plt.xlabel('date hr/(hr)');\n",
    "        plt.ylabel('consumption');\n",
    "        plt.title(t,loc='center');\n",
    "        plt.grid();\n",
    "        plt.show();\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot of prediction vs actual reading\n",
    "valid_id=all_result['dataid'].unique()\n",
    "for _id in valid_id:\n",
    "    temp=all_result[all_result['dataid']==_id];\n",
    "    fig=plt.figure(figsize=(10,10));\n",
    "    t='Prediction vs Actual of '+str(_id)+' with fit line(y=x)'\n",
    "    plt.scatter(temp.predicted_reading.tolist(),temp.actual_reading.tolist());\n",
    "    plt.plot(temp.predicted_reading,temp.predicted_reading,'r',label='y=x')\n",
    "    plt.legend();\n",
    "    plt.xlabel('Predicted reading');\n",
    "    plt.ylabel('Actual reading');\n",
    "    plt.title(t,loc='center');\n",
    "    plt.grid();\n",
    "    plt.show();\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_array=[];\n",
    "score=[];\n",
    "decre_index=0;\n",
    "id_list1=[35]\n",
    "all_result=pd.DataFrame(columns=['localminute','dataid','actual_reading','predicted_reading']);\n",
    "\n",
    "for index,item in enumerate(id_list):\n",
    "    #Extract attributes and wanted values\n",
    "    X_temp=normed_gas_hr[normed_gas_hr['dataid']==item][['localminute']];\n",
    "    Y_temp=normed_gas_hr[normed_gas_hr['dataid']==item][['meter_value']];\n",
    "    #Split train and test case\n",
    "    (X_train, X_test, Y_train, Y_test)=model_sel.train_test_split(X_temp,Y_temp,test_size=0.1);\n",
    "    \n",
    "    #For lacking ids add in more training data\n",
    "    if(item in id_lack):\n",
    "        X_train=X_train.append(normed_gas_hr_cp[normed_gas_hr_cp['dataid']==item][['localminute']]);\n",
    "        Y_train=Y_train.append(normed_gas_hr_cp[normed_gas_hr_cp['dataid']==item][['meter_value']]);\n",
    "    \n",
    "    # Reshape Y\n",
    "    Y_train = Y_train.meter_value.ravel()\n",
    "    \n",
    "    #only train for model has more than 10 data points, since the model will be more or less meaningless with \n",
    "    #very little amout of data\n",
    "    if(len(X_temp)<10):\n",
    "        P_array.append('not enough data');\n",
    "        score.append('not enough data');\n",
    "    else:\n",
    "        #Training using SVR model\n",
    "        LR = SVR(C = 3000, epsilon = 20, gamma = 1e-5)\n",
    "        predictor=LR.fit(X_train,Y_train);\n",
    "        #Store the trained predictor and the score of the prefictor for future use\n",
    "        P_array.append(predictor)\n",
    "        score.append(P_array[index].score(X_test,Y_test));\n",
    "        #Predict for test data for comparison\n",
    "        predicted=P_array[index].predict(X_test);\n",
    "        \n",
    "        #Convert back the time number to time point\n",
    "        #Convert back consumption to meter readings\n",
    "        X_test=X_test.localminute.map(lambda x: dt.timedelta(hours=x)+min_time);\n",
    "        Y_test=Y_test.meter_value.map(lambda x:x+min_consump[index]);\n",
    "        predicted=np.add(predicted,min_consump[index]);\n",
    "        result=pd.DataFrame(columns=['localminute','dataid','actual_reading','predicted_reading']);\n",
    "        result['localminute']=X_test;\n",
    "        result['actual_reading']=Y_test;\n",
    "        result['predicted_reading']=predicted;\n",
    "        result['dataid']=item;\n",
    "        result=result.sort_values(by='localminute');\n",
    "        #Store the predicted result for plots in the next section\n",
    "        all_result = all_result.append(result)\n",
    "        \n",
    "        #Plot of prediction vs actual reading against time\n",
    "        fig=plt.figure(figsize=(10,10));\n",
    "        t='Prediction & Actual reading vs time of '+str(item)\n",
    "        plt.plot(result.localminute,result.actual_reading,'r',label='Actual');\n",
    "        plt.plot(result.localminute,result.predicted_reading,'b',label='Predicted');\n",
    "        plt.legend()\n",
    "        plt.xlabel('date hr/(hr)');\n",
    "        plt.ylabel('consumption');\n",
    "        plt.title(t,loc='center');\n",
    "        plt.grid();\n",
    "        plt.show();\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot of prediction vs actual reading\n",
    "valid_id=all_result['dataid'].unique()\n",
    "for _id in valid_id:\n",
    "    temp=all_result[all_result['dataid']==_id];\n",
    "    fig=plt.figure(figsize=(10,10));\n",
    "    t='Prediction vs Actual of '+str(_id)+' with fit line(y=x)'\n",
    "    plt.scatter(temp.predicted_reading.tolist(),temp.actual_reading.tolist());\n",
    "    plt.plot(temp.predicted_reading,temp.predicted_reading,'r',label='y=x')\n",
    "    plt.legend();\n",
    "    plt.xlabel('Predicted reading');\n",
    "    plt.ylabel('Actual reading');\n",
    "    plt.title(t,loc='center');\n",
    "    plt.grid();\n",
    "    plt.show();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning parameters\n",
    "parameters = [{'kernel': ['rbf'],'gamma': [1e-4, 1e-5], 'C': [2000, 3000], 'epsilon' : [50, 100]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation\n",
    "\n",
    "clf = GridSearchCV(SVR(), parameters, cv=5)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on training set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As seen from the plots, the performance of the LR predictor is not very good. That is mainly because many id's consumption pattern is not linear in nature, but rather looks like a polynomial. Hence by applying LR to predict, the outcome is not very satisfactory. \n",
    "\n",
    "In comparison, the performance of SVR is much better than LR as it is able to separate linearly non separable data by doing the kernel trick at higher dimensions, as well as controlling how close the fit is the model to the data, resulting in a closely represented model of the data. Parameters of SVR is selected based with 5-fold cross validation, and visual inspection of the resultant graph. As it is observed that the validation will try to fit to the graph very closely, we decide to stop at a certain which we feel that tuning the parameters more will only result in overfitting. Else it will only result in a very large C value, very large epsilon value and a small gamma value.\n",
    "\n",
    "In conclusion, SVR is more robust at predicting data compared to LR if the data is non linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3 Clustering and analysis of residential customers’ gas consumption behavioral using aggregated smart meter data</h1>\n",
    "\n",
    "\n",
    "Following topics will be discussied: introduction, feature extraction, analysis on number of clusters chosen, k-means clustering implementation, user behavior analysis based on clustering, applying price discrimination on different clusters and conclusion.\n",
    "<h2>3.1 Introduction</h2> \n",
    "\n",
    "The wide spread of smart metering roll out enables a better understanding of the consumer behavior and tailoring demand response to achieve cost-efficient energy savings. In the residential sector smart metering allows detailed readings of the power consumption in the form of large volumes time series that encodes relevant information to be found. In our project, we would like to analyze the gas consumption data further by applying clustering method to measure the similarity between the households, and group them based on energy usage. The goal in clustering time‐series data is to understand user behavior by organizing the data into homogeneous groups, maximizing the similarity and dissimilarity within and between groups, respectively.    \n",
    "\n",
    "Clustering the most important attributes (more in section 3.2 feature extraction) of customers is a very common method for better understanding the different residential energy behaviours that exist and has many applications (Stephen,2015). Potential application areas for work using this form of cluster analysis as more smart data becomes routinely available include\n",
    "1. Application of cluster results for segment-specific rate design: the economic benefits of time-variable natural gas rates consist in the potential for utilities to improve price discrimination and to facilitate the reduction of peak loads. A complete design of gas scheme would encompass number of different time zones and starting time of each period. All these could be done through clustering analysis. In addition, by influencing the demand side, segment-specific natural gas rates can improve the energy system’s efficiency.\n",
    "\n",
    "2. Application of cluster results for leak detection activities based on detecting pattern changes (deviation from cluster centroids/ distributions); data-driven models of demand could also help identify atypical customers or unusual changes in consumption. \n",
    "\n",
    "3. Application of cluster results for filling missing data for audits/ regulatory purposes using cluster centroids. Typical usage perhaps allows volumetric usage and flow profiles to be estimated for unmetered customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.2 Feature extraction</h2>\n",
    "\n",
    "Clustering is a collection of same group similar or data objects or in other groups. Its also finding the dissimilar to the data objects in other groups. In cluster analysis, the main objective is to find similarities between data objects with the help of specific characteristics found in the data and grouping these similar data objects into clusters. Large quantities of information about how customers use their energy is then becoming available through the uptake of smart meters. In our project, the following features and their respective standard deviation will be examined in the following clustering discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Features</th>\n",
    "<th>Discription</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Total consumption</td>\n",
    "<td>Total consumed power during period of the study</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Morning consumtption</td>\n",
    "<td>The consumption observed from 06:00 - 09:00</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Noon consumtption</td>\n",
    "<td>The consumption observed from 11:00 - 14:00</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Night consumtption</td>\n",
    "<td>The consumption observed from 17:00 - 20:00</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "import timeit\n",
    "import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import HTML\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a csv file from hourly_readings_final.csv for easier computation in the later sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import result obtained from 1.2\n",
    "hourly_reading = pd.read_csv('./csv/hourly_readings_final.csv')\n",
    "\n",
    "# split the column 'localminute' into 'day' and 'time'\n",
    "new_time = hourly_reading['localminute'].str.split(\" \", n = 1, expand = True)\n",
    "hourly_reading['day'] = new_time[0]\n",
    "hourly_reading['time'] = new_time[1]\n",
    "hourly_reading.drop(columns = ['localminute'], inplace = True)\n",
    "\n",
    "# add the 'consumption' column which shows the hourly consumption by each household\n",
    "# the function diff() computes the difference between the current data point and the previous data point\n",
    "# therefore the data points at the boundary (data id changes) are wrong and will be changed to 0\n",
    "hourly_reading['consumption'] = hourly_reading['meter_value'].diff()\n",
    "mask = (hourly_reading.dataid != hourly_reading.dataid.shift(1))\n",
    "hourly_reading.loc[mask, 'consumption'] = 0\n",
    "hourly_reading.to_csv(path+\"/hourly_readings_with_consumption.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers include all features needed for later discussion\n",
    "df = pd.read_csv(path+\"/hourly_readings_with_consumption.csv\")\n",
    "\n",
    "headers = [\"dataid\", \"totalConsumption\", \"morningTotal\",\"morningMean\", \"morningSTD\", \"noonTotal\",\"noonMean\",\n",
    "          \"noonSTD\", \"nightTotal\",\"nightMean\", \"nightSTD\",\"elseConsumption\"]\n",
    "currentID = df.iloc[0]['dataid']\n",
    "baseTime = df.iloc[0]['time'] \n",
    "featuresList = []\n",
    "morningConsumption = []\n",
    "noonConsumption = []\n",
    "nightConsumption = []\n",
    "totalConsumption = 0\n",
    "elseConsumption = []\n",
    "currentMorning = 0\n",
    "currentNoon = 0\n",
    "currentNight = 0\n",
    "currentElse = 0\n",
    "prevDay = df.iloc[0]['day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction \n",
    "for row in df.itertuples():\n",
    "    currentTime = row.time\n",
    "    hrs_diff = int(currentTime.split(\":\")[0][:2])\n",
    "    currentDay = row.day\n",
    "    \n",
    "    # For different days, we calculate their morning, noon and night consumption \n",
    "    if (currentDay != prevDay):\n",
    "        morningConsumption.append(currentMorning)\n",
    "        noonConsumption.append(currentNoon)\n",
    "        nightConsumption.append(currentNight)\n",
    "        elseConsumption.append(currentElse)\n",
    "        prevDay = currentDay\n",
    "        currentMorning = 0\n",
    "        currentNoon = 0\n",
    "        currentNight = 0\n",
    "        currentElse = 0\n",
    "    \n",
    "    # For different ID, we append respective features to the list \n",
    "    if (currentID != row.dataid):\n",
    "        features = []\n",
    "        morningTotal = np.sum(morningConsumption)\n",
    "        morningMean = np.mean(morningConsumption)\n",
    "        morningSTD = np.std(morningConsumption)\n",
    "        \n",
    "        noonTotal = np.sum(noonConsumption)\n",
    "        noonMean = np.mean(noonConsumption)\n",
    "        noonSTD = np.std(noonConsumption)\n",
    "        \n",
    "        nightTotal = np.sum(nightConsumption)\n",
    "        nightMean = np.mean(nightConsumption)\n",
    "        nightSTD = np.std(nightConsumption)\n",
    "        elseConsumptionTotal = np.sum(elseConsumption)\n",
    "        \n",
    "        features.append(currentID)\n",
    "        features.append(totalConsumption)\n",
    "        features.append(morningTotal)\n",
    "        features.append(morningMean)\n",
    "        features.append(morningSTD)\n",
    "        features.append(noonTotal)\n",
    "        features.append(noonMean)\n",
    "        features.append(noonSTD)\n",
    "        features.append(nightTotal)\n",
    "        features.append(nightMean)\n",
    "        features.append(nightSTD)\n",
    "        \n",
    "        features.append(elseConsumptionTotal)\n",
    "        featuresList.append(features)\n",
    "        \n",
    "        elseConsumption =[]\n",
    "        morningConsumption = []\n",
    "        noonConsumption = []\n",
    "        nightConsumption = []\n",
    "        totalConsumption = 0\n",
    "        currentID = row.dataid\n",
    "    \n",
    "    # Feature calculation\n",
    "    totalConsumption = totalConsumption + int(row.consumption)\n",
    "    if (hrs_diff >= 6 and hrs_diff <= 9):\n",
    "        currentMorning = currentMorning + int(row.consumption)\n",
    "    elif (hrs_diff >= 11 and hrs_diff <= 14):\n",
    "        currentNoon = currentNoon + int(row.consumption)\n",
    "    elif (hrs_diff >= 17 and hrs_diff <= 20):\n",
    "        currentNight = currentNight + int(row.consumption)\n",
    "    else:\n",
    "        currentElse = currentElse + int(row.consumption)\n",
    "\n",
    "morningConsumption.append(currentMorning)\n",
    "noonConsumption.append(currentNoon)\n",
    "nightConsumption.append(currentNight)\n",
    "morningMean = np.mean(morningConsumption)\n",
    "morningSTD = np.std(morningConsumption)\n",
    "noonMean = np.mean(noonConsumption)\n",
    "noonSTD = np.std(noonConsumption)\n",
    "nightMean = np.mean(nightConsumption)\n",
    "nightSTD = np.std(nightConsumption)\n",
    "features = []\n",
    "\n",
    "# We append the last ID features\n",
    "features.append(currentID)\n",
    "features.append(totalConsumption)\n",
    "features.append(morningTotal)\n",
    "features.append(morningMean)\n",
    "features.append(morningSTD)\n",
    "features.append(noonTotal)\n",
    "features.append(noonMean)\n",
    "features.append(noonSTD)\n",
    "features.append(nightTotal)\n",
    "features.append(nightMean)\n",
    "features.append(nightSTD)\n",
    "features.append(elseConsumptionTotal)\n",
    "featuresList.append(features)\n",
    "\n",
    "featureFrame = pd.DataFrame(featuresList)\n",
    "featureFrame.to_csv(path + '/reading_features.csv', header=headers, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature values are normalized to values between 0 and 1 in order to stabilize the algorithm to differences in scale across features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation of feature values\n",
    "df = pd.read_csv(path+\"/reading_features.csv\")\n",
    "featureNames = [\"totalConsumption\", \"morningTotal\", \"noonTotal\", \"nightTotal\"]\n",
    "# featureNames = [\"totalConsumption\", \"morningMean\", \"noonMean\", \"nightMean\"]\n",
    "\n",
    "# Normalise the data for range o to 1\n",
    "x = df[featureNames].values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df_normalized = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.3 Analysis on number of clusters chosen</h2>\n",
    "\n",
    "In order to determine number of clusters we should have, silhouette coefficient is used to quantify the quality of clustering achieved. We will select the number of clusters that maximizes the silhouette coefficient. The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette coefficient is calculated as followed and the calculation is implemented in the code below. For each point p, first find the average distance between p and all other points in the same cluster (this is a measure of cohesion, call it A). Then find the average distance between p and all points in the nearest cluster (this is a measure of separation from the closest other cluster, call it B). The silhouette coefficient for p is defined as the difference between B and A divided by the greater of the two (max (A, B)). Figure above provides a silhouette plot for k-means applied with different cluster numbers ranging from 2 to 6. From all the plots below, 3 clusters have a highest average silhouette score which is the optimum solution in our case and this will be used for further discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We test on different cluster numbers: 2, 3, 4, 5 and 6\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "X= df_normalized.values\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # In our example, the silhouette coefficient can range from -0.1. The higher, the better.\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the combined 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the combined 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.4 K-means clustering implementation</h2>\n",
    "\n",
    "K-medoids clustering selects the most centrally located data points within clusters as cluster centers called medoids and returns the number of clustering as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3).fit(df_normalized)\n",
    "df.insert(loc=0, column=\"classification\", value=kmeans.labels_)\n",
    "df.to_csv(path + '/reading_features_classification.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Visualization of Data plots  \n",
    "fig, (ax3, ax4,ax5) = plt.subplots(1, 3)\n",
    "fig.set_size_inches(22, 7)\n",
    "\n",
    "df_array = df.values\n",
    "ax3.scatter(df_array[:,2],df_array[:,4], c=kmeans.labels_, cmap='rainbow')  \n",
    "ax3.set_title(\"The visualization of the clustered data.\")\n",
    "ax3.set_xlabel('Total Consumption')\n",
    "ax3.set_ylabel('Morning Average Consumption')\n",
    "\n",
    "ax4.scatter(df_array[:,2],df_array[:,7], c=kmeans.labels_, cmap='rainbow') \n",
    "ax4.set_title(\"The visualization of the clustered data.\")\n",
    "ax4.set_xlabel('Total Consumption')\n",
    "ax4.set_ylabel('Noon Average Consumption')\n",
    "\n",
    "ax5.scatter(df_array[:,2],df_array[:,10], c=kmeans.labels_, cmap='rainbow')  \n",
    "ax5.set_title(\"The visualization of the clustered data.\")\n",
    "ax5.set_xlabel('Total Consumption')\n",
    "ax5.set_ylabel('Night Average Consumption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all 4 feature into 2 feature sets\n",
    "pca = PCA(n_components=2).fit(df[featureNames])\n",
    "pca_2d = pca.transform(df[featureNames])\n",
    "for i in range(0, pca_2d.shape[0]):\n",
    "    if df.classification[i] == 0:\n",
    "        c1 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='purple',marker='+')\n",
    "    elif df.classification[i] == 1:\n",
    "        c2 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='cyan',marker='o')\n",
    "    elif df.classification[i] == 2:\n",
    "        c3 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='red',marker='*')\n",
    "pl.legend([c1, c2, c3], ['C0', 'C1','C2'])\n",
    "pl.title('Gas consumption dataset with 3 clusters and knownoutcomes')\n",
    "pl.xlabel('First reduced feature set')\n",
    "pl.ylabel('Second reduced feature set')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.5 User behavior analysis based on clustering</h2>\n",
    "\n",
    "\n",
    "From the plot above, we able to see that there are three main types of pattern in the dataset: a profile with the typical high morning, noon and night gas consumption (cluster 0), a profile with constant low gas consumption (cluster 1) and finally a profile that has high consumption with no distinct peak hours (cluster 2). \n",
    "\n",
    "Cluster 0 grouping has 3 distinct peak usage expected at morning, noon and night time. Data has shown that most consumption of gas happen on morning period (06:00 - 09:00) period and second most consumption happen on the night (17:00 - 20:00).\n",
    "\n",
    "Cluster 1 groupings have a very low utility rate of natural gas. Those households may be the business man or woman who usually do not cook at home. Those households are predicted to be of the working age 35 to 50 and are the main contribution to the country’s economy. \n",
    "\n",
    "Cluster 2 grouping seems to indicate a relatively high consumption pattern with a householder generally at home throughout the day, whilst not possessing distinct morning and evening demand peaks, and there is not a well delineated minimum in between them. Demographics such as age and employment status in particular regions could contribute to this cluster. For example, most of households may be the elderly or jobless people need to stay at home. \n",
    "\n",
    "In general, data clustering is used to find similar groups having the same gas consumption patterns. Smart meter data clustering has been widely investigated toward consumers grouping and revealing their energy usage behavior which leads to more efficient tariff policy and tailored energy efficiency programs for specific users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.6 Applying price discrimination on different clusters</h2>\n",
    "\n",
    "After computing the clusters of households based on their gas consumption patterns, we can adjust the pricing strategies for different clusters in order to modify their gas consumption behaviours, to limit peak load and to increase natural gas demand during non-peak hours. By doing so, gas companies can reduce the pressure on the natural gas transportation facilities such as the pipelines during peak hours (morning, noon and night as stated in the earlier sections), and the company can possibly reap more benefits by adjusting the pricing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path().resolve().parent)+'\\csv'\n",
    "df = pd.read_csv(path + '/reading_features_classification.csv')\n",
    "hourly_reading = pd.read_csv(path + '/hourly_readings_with_consumption.csv')\n",
    "\n",
    "# monthly price of residential gas in Texas from Oct 2015 to Mar 2016\n",
    "# data retrieved from: https://www.eia.gov/dnav/ng/hist/n3010tx3m.htm\n",
    "monthly_price = [19.22, 15.52, 9.33, 7.85, 7.87, 10.12]\n",
    "\n",
    "# compute the monthly consumption for each household and add the columns to store these values for future computation\n",
    "new = hourly_reading['day'].str.split(\"-\", n = 2, expand = True)\n",
    "hourly_reading.drop(columns = ['day'], inplace = True)\n",
    "hourly_reading['month'] = new[1]\n",
    "df = df.assign(oct_consumption = np.random.randn(len(df)), nov_consumption = np.random.randn(len(df)), dec_consumption = np.random.randn(len(df)), jan_consumption = np.random.randn(len(df)), feb_consumption = np.random.randn(len(df)), mar_consumption = np.random.randn(len(df)))\n",
    "for i in range (0,len(df)):\n",
    "    df.loc[i,'oct_consumption'] = hourly_reading['consumption'][(hourly_reading.dataid == df.loc[i, 'dataid']) & (hourly_reading.month == '10')].sum()\n",
    "    df.loc[i,'nov_consumption'] = hourly_reading['consumption'][(hourly_reading.dataid == df.loc[i, 'dataid']) & (hourly_reading.month == '11')].sum()\n",
    "    df.loc[i,'dec_consumption'] = hourly_reading['consumption'][(hourly_reading.dataid == df.loc[i, 'dataid']) & (hourly_reading.month == '12')].sum()\n",
    "    df.loc[i,'jan_consumption'] = hourly_reading['consumption'][(hourly_reading.dataid == df.loc[i, 'dataid']) & (hourly_reading.month == '01')].sum()\n",
    "    df.loc[i,'feb_consumption'] = hourly_reading['consumption'][(hourly_reading.dataid == df.loc[i, 'dataid']) & (hourly_reading.month == '02')].sum()\n",
    "    df.loc[i,'mar_consumption'] = hourly_reading['consumption'][(hourly_reading.dataid == df.loc[i, 'dataid']) & (hourly_reading.month == '03')].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below calculates the total cost of natural gas consumed by all the households during the 6 month period from Oct 2015 to Mar 2016 before and after adjusting the pricing strategy.\n",
    "\n",
    "Instead of changing the price monthly, the average price for the 6 month is calculated as the base amount, and the price for each cluster in different time period will change based on the average price.\n",
    "\n",
    "Households in cluster 0 (classification == 0) use moderately high amount of gas, and peak of gas consumption happens in the morning (06:00 - 09:00) as well as at night (17:00 - 20:00). The aim will be to reduce their gas consumption in the morning and at night. Threshold values for morning, noon and night consumption are set as the desired gas consumption amount for each household in the given time periods. If the households consume more than the threshold during morning and night, the exceeding amount will be charged more than the average price. Otherwise the price will remain as the average price.\n",
    "\n",
    "For households in cluster 1 (classification == 1), they generally use very little amount of gas, so the aim is to encourage them to consume more natural gas. Different threshold values, which are lower than the thresholds for cluster 0 and 2, are set, and if the households consume more than the threshold value in certain time periods, the price of natural gas will be lower than the average price.\n",
    "\n",
    "Households in cluster 2 (classification == 2) consume significantly greater amount of natural gas compared to other households. Therefore the pricing strategy for cluster 2 will be the same as cluster 0, except that the increase in price is much higher, as a small increase in price is unlikely to have much impact on their behaviours. In addition, if they consume less than the threshold values in either of the three time periods, the price will be lower than the average price as an encouragement for the households to consume less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and output the original cost of natural gas consumed by all households based on the monthly price found in the website\n",
    "total_cost = 0\n",
    "for row in df.itertuples():\n",
    "    total_cost += (row.oct_consumption * monthly_price[0] + row.nov_consumption * monthly_price[1] + row.dec_consumption * monthly_price[2] + row.jan_consumption * monthly_price[3] + row.feb_consumption * monthly_price[4] + row.mar_consumption * monthly_price[5])\n",
    "print(\"Original cost of natural gas: %.2f dollars\" % (total_cost/1000))\n",
    "\n",
    "# compute the average price\n",
    "total_consumption = 0\n",
    "for row in df.itertuples():\n",
    "    total_consumption += row.totalConsumption\n",
    "avg_price = total_cost / total_consumption\n",
    "\n",
    "# set threshold values\n",
    "morningThreshold = 3000\n",
    "noonThreshold = 2400\n",
    "nightThreshold = 4200\n",
    "\n",
    "morningThreshold_2 = 2400\n",
    "noonThreshold_2 = 1800\n",
    "nightThreshold_2 = 3000\n",
    "\n",
    "# compute the new estimated cost after changing the pricing strategy\n",
    "total_adjusted_cost = 0\n",
    "for row in df.itertuples():\n",
    "    if (row.classification == 0):\n",
    "        if (row.morningTotal > morningThreshold):\n",
    "            total_adjusted_cost += (row.morningTotal - morningThreshold) * (avg_price + 2) + morningThreshold * avg_price\n",
    "        else:\n",
    "            total_adjusted_cost += row.morningTotal * avg_price\n",
    "        total_adjusted_cost += row.noonTotal * avg_price\n",
    "        if (row.nightTotal > nightThreshold):\n",
    "            total_adjusted_cost += (row.nightTotal - nightThreshold) * (avg_price + 2) + nightThreshold * avg_price\n",
    "        else:\n",
    "            total_adjusted_cost += row.nightTotal * avg_price\n",
    "        \n",
    "    if (row.classification == 1):\n",
    "        if (row.morningTotal > morningThreshold_2):\n",
    "            total_adjusted_cost += row.morningTotal * (avg_price - 1)\n",
    "        else:\n",
    "            total_adjusted_cost += row.morningTotal * avg_price\n",
    "        if (row.noonTotal > noonThreshold_2):\n",
    "            total_adjusted_cost += row.noonTotal * (avg_price - 1)\n",
    "        else:\n",
    "            total_adjusted_cost += row.noonTotal * avg_price\n",
    "        if (row.nightTotal > nightThreshold_2):\n",
    "            total_adjusted_cost += row.nightTotal * (avg_price - 1)\n",
    "        else:\n",
    "            total_adjusted_cost += row.nightTotal * avg_price\n",
    "        \n",
    "    if (row.classification == 2):\n",
    "        if (row.morningTotal > morningThreshold):\n",
    "            total_adjusted_cost += (row.morningTotal - morningThreshold) * (avg_price + 4) + morningThreshold * avg_price\n",
    "        else:\n",
    "            total_adjusted_cost += row.morningTotal * (avg_price - 1)\n",
    "        if (row.noonTotal > noonThreshold):\n",
    "            total_adjusted_cost += (row.noonTotal - noonThreshold) * (avg_price + 4) + noonThreshold * avg_price\n",
    "        else:\n",
    "            total_adjusted_cost += row.noonTotal * (avg_price - 1)\n",
    "        if (row.nightTotal > nightThreshold):\n",
    "            total_adjusted_cost += (row.nightTotal - nightThreshold) * (avg_price + 4) + nightThreshold * avg_price\n",
    "        else:\n",
    "            total_adjusted_cost += row.nightTotal * (avg_price - 1)\n",
    "        \n",
    "    total_adjusted_cost += (row.totalConsumption - row.morningTotal - row.noonTotal - row.nightTotal) * avg_price\n",
    "print(\"Estimated cost after changing the price: %.2f dollars\" % (total_adjusted_cost/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.7 Conclusion</h2>\n",
    "\n",
    "In our project, investigations have been performed into using clustering methodsin data mining time‐series data from smart meters. The problem is to identify patterns and trends in gas consumption profiles of residential customers over a large period of time, and group similar profiles into 3 clusters. The results has shown accurate grouping of accounts similar in their energy usage patterns, and potential for adjusting the pricing strategies for different clusters in order to modify their gas consumption behaviours for higher profits and better energy efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
